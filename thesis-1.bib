
@article{hindman_mesos_nodate,
	title = {Mesos: {A} {Platform} for {Fine}-{Grained} {Resource} {Sharing} in the {Data} {Center}},
	abstract = {We present Mesos, a platform for sharing commodity clusters between multiple diverse cluster computing frameworks, such as Hadoop and MPI. Sharing improves cluster utilization and avoids per-framework data replication. Mesos shares resources in a ﬁne-grained manner, allowing frameworks to achieve data locality by taking turns reading data stored on each machine. To support the sophisticated schedulers of today’s frameworks, Mesos introduces a distributed two-level scheduling mechanism called resource offers. Mesos decides how many resources to offer each framework, while frameworks decide which resources to accept and which computations to run on them. Our results show that Mesos can achieve near-optimal data locality when sharing the cluster among diverse frameworks, can scale to 50,000 (emulated) nodes, and is resilient to failures.},
	language = {en},
	author = {Hindman, Benjamin and Konwinski, Andy and Zaharia, Matei and Ghodsi, Ali and Joseph, Anthony D and Katz, Randy and Shenker, Scott and Stoica, Ion},
	file = {Hindman et al. - Mesos A Platform for Fine-Grained Resource Sharin.pdf:/Users/hamza/Zotero/storage/I8RVS2JD/Hindman et al. - Mesos A Platform for Fine-Grained Resource Sharin.pdf:application/pdf},
}

@misc{noauthor_mercedes-benz_2023,
	title = {Mercedes-{Benz}},
	url = {https://www.cncf.io/case-studies/mercedes-benz/},
	abstract = {Mercedes-Benz is one of the most successful automotive companies in the world. As a 100\% subsidiary of Mercedes-Benz, Mercedes-Benz Tech Innovation GmbH creates digital products and software solutions…},
	language = {en-US},
	urldate = {2023-07-31},
	journal = {Cloud Native Computing Foundation},
	month = jun,
	year = {2023},
}

@misc{noauthor_dynamic_nodate,
	title = {Dynamic {Kubernetes} {Cluster} {Scaling} at {Airbnb} {\textbar} by {David} {Morrison} {\textbar} {The} {Airbnb} {Tech} {Blog} {\textbar} {Medium}},
	url = {https://medium.com/airbnb-engineering/dynamic-kubernetes-cluster-scaling-at-airbnb-d79ae3afa132},
	urldate = {2023-07-31},
}

@inproceedings{zaharia_delay_2010,
	address = {Paris France},
	title = {Delay scheduling: a simple technique for achieving locality and fairness in cluster scheduling},
	isbn = {978-1-60558-577-2},
	shorttitle = {Delay scheduling},
	url = {https://dl.acm.org/doi/10.1145/1755913.1755940},
	doi = {10.1145/1755913.1755940},
	abstract = {As organizations start to use data-intensive cluster computing systems like Hadoop and Dryad for more applications, there is a growing need to share clusters between users. However, there is a conﬂict between fairness in scheduling and data locality (placing tasks on nodes that contain their input data). We illustrate this problem through our experience designing a fair scheduler for a 600-node Hadoop cluster at Facebook. To address the conﬂict between locality and fairness, we propose a simple algorithm called delay scheduling: when the job that should be scheduled next according to fairness cannot launch a local task, it waits for a small amount of time, letting other jobs launch tasks instead. We ﬁnd that delay scheduling achieves nearly optimal data locality in a variety of workloads and can increase throughput by up to 2x while preserving fairness. In addition, the simplicity of delay scheduling makes it applicable under a wide variety of scheduling policies beyond fair sharing.},
	language = {en},
	urldate = {2023-08-02},
	booktitle = {Proceedings of the 5th {European} conference on {Computer} systems},
	publisher = {ACM},
	author = {Zaharia, Matei and Borthakur, Dhruba and Sen Sarma, Joydeep and Elmeleegy, Khaled and Shenker, Scott and Stoica, Ion},
	month = apr,
	year = {2010},
	pages = {265--278},
	file = {Zaharia et al. - 2010 - Delay scheduling a simple technique for achieving.pdf:/Users/hamza/Zotero/storage/STFXM2WK/Zaharia et al. - 2010 - Delay scheduling a simple technique for achieving.pdf:application/pdf},
}

@inproceedings{patel_what_2022,
	title = {What does {Inter}-{Cluster} {Job} {Submission} and {Execution} {Behavior} {Reveal} to {Us}?},
	doi = {10.1109/CLUSTER51413.2022.00019},
	abstract = {Modern High Performing Computing (HPC) facil-ities have multiple computing clusters that serve different pur-poses. These include large-scale computing clusters and smaller data visualization and analysis clusters, which are meant to shift the load of data analytics jobs from the large-scale systems. We perform the first in-depth characterization of cross-cluster behavior of users and jobs and provide an analysis of three inter-related systems at the Argonne Leadership Computing Facility (ALCF). Our analysis reveals interesting trends related to the resource utilization and predictability of user and job behavior across different clusters.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Cluster} {Computing} ({CLUSTER})},
	author = {Patel, Tirthak and Tiwari, Devesh and Kettimuthu, Raj and Allcock, William and Rich, Paul and Liu, Zhengchun},
	month = sep,
	year = {2022},
	note = {ISSN: 2168-9253},
	keywords = {Predictive models, Cross-Cluster Analysis, Data analysis, Data centers, Data models, Data visualization, High-Performance Computing, Large-Scale Systems, Leadership, Market research, Supercomputing},
	pages = {35--46},
	file = {IEEE Xplore Abstract Record:/Users/hamza/Zotero/storage/NMN8LBMR/stamp.html:text/html;IEEE Xplore Full Text PDF:/Users/hamza/Zotero/storage/YCYSEJ9B/Patel et al. - 2022 - What does Inter-Cluster Job Submission and Executi.pdf:application/pdf},
}

@inproceedings{lo_heracles_2015,
	address = {Portland Oregon},
	title = {Heracles: improving resource efficiency at scale},
	isbn = {978-1-4503-3402-0},
	shorttitle = {Heracles},
	url = {https://dl.acm.org/doi/10.1145/2749469.2749475},
	doi = {10.1145/2749469.2749475},
	abstract = {User-facing, latency-sensitive services, such as websearch, underutilize their computing resources during daily periods of low trafﬁc. Reusing those resources for other tasks is rarely done in production services since the contention for shared resources can cause latency spikes that violate the service-level objectives of latency-sensitive tasks. The resulting under-utilization hurts both the affordability and energy-efﬁciency of large-scale datacenters. With technology scaling slowing down, it becomes important to address this opportunity.},
	language = {en},
	urldate = {2023-09-19},
	booktitle = {Proceedings of the 42nd {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {ACM},
	author = {Lo, David and Cheng, Liqun and Govindaraju, Rama and Ranganathan, Parthasarathy and Kozyrakis, Christos},
	month = jun,
	year = {2015},
	pages = {450--462},
	file = {Lo et al. - 2015 - Heracles improving resource efficiency at scale.pdf:/Users/hamza/Zotero/storage/4MPWHPR7/Lo et al. - 2015 - Heracles improving resource efficiency at scale.pdf:application/pdf},
}

@misc{wilkes_yet_2020,
	title = {Yet more {Google} compute cluster trace data},
	author = {Wilkes, John},
	month = apr,
	year = {2020},
	note = {Place: Mountain View, CA, USA
Published: Google research blog},
	annote = {Posted at https://ai.googleblog.com/2020/04/yet-more-google-compute-cluster-trace.html.},
}

@inproceedings{bhattacharya_hierarchical_2013,
	address = {Santa Clara California},
	title = {Hierarchical scheduling for diverse datacenter workloads},
	isbn = {978-1-4503-2428-1},
	url = {https://dl.acm.org/doi/10.1145/2523616.2523637},
	doi = {10.1145/2523616.2523637},
	abstract = {There has been a recent industrial effort to develop multi-resource hierarchical schedulers. However, the existing implementations have some shortcomings in that they might leave resources unallocated or starve certain jobs. This is because the multi-resource setting introduces new challenges for hierarchical scheduling policies. We provide an algorithm, which we implement in Hadoop, that generalizes the most commonly used multi-resource scheduler, DRF [1], to support hierarchies. Our evaluation shows that our proposed algorithm, H-DRF, avoids the starvation and resource inefﬁciencies of the existing open-source schedulers and outperforms slot scheduling.},
	language = {en},
	urldate = {2023-01-19},
	booktitle = {Proceedings of the 4th annual {Symposium} on {Cloud} {Computing}},
	publisher = {ACM},
	author = {Bhattacharya, Arka A. and Culler, David and Friedman, Eric and Ghodsi, Ali and Shenker, Scott and Stoica, Ion},
	month = oct,
	year = {2013},
	pages = {1--15},
	file = {Bhattacharya et al. - 2013 - Hierarchical scheduling for diverse datacenter wor.pdf:/Users/hamza/Zotero/storage/L6M67DXV/Bhattacharya et al. - 2013 - Hierarchical scheduling for diverse datacenter wor.pdf:application/pdf},
}

@inproceedings{li_lyra_2023,
	address = {Rome Italy},
	title = {Lyra: {Elastic} {Scheduling} for {Deep} {Learning} {Clusters}},
	isbn = {978-1-4503-9487-1},
	shorttitle = {Lyra},
	url = {https://dl.acm.org/doi/10.1145/3552326.3587445},
	doi = {10.1145/3552326.3587445},
	language = {en},
	urldate = {2023-09-19},
	booktitle = {Proceedings of the {Eighteenth} {European} {Conference} on {Computer} {Systems}},
	publisher = {ACM},
	author = {Li, Jiamin and Xu, Hong and Zhu, Yibo and Liu, Zherui and Guo, Chuanxiong and Wang, Cong},
	month = may,
	year = {2023},
	pages = {835--850},
	file = {Full Text PDF:/Users/hamza/Zotero/storage/5JU3FQPZ/Li et al. - 2023 - Lyra Elastic Scheduling for Deep Learning Cluster.pdf:application/pdf},
}

@misc{noauthor_cncf_2023,
	title = {{CNCF} {Annual} {Survey} 2022},
	url = {https://www.cncf.io/reports/cncf-annual-survey-2022/},
	language = {en-US},
	urldate = {2023-09-19},
	journal = {Cloud Native Computing Foundation},
	month = jan,
	year = {2023},
	file = {Snapshot:/Users/hamza/Zotero/storage/W6MRZP49/cncf-annual-survey-2022.html:text/html},
}

@inproceedings{verma_large-scale_2015,
	address = {Bordeaux, France},
	title = {Large-scale cluster management at {Google} with {Borg}},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Systems} ({EuroSys})},
	author = {Verma, Abhishek and Pedrosa, Luis and Korupolu, Madhukar R. and Oppenheimer, David and Tune, Eric and Wilkes, John},
	year = {2015},
}

@misc{noauthor_multi-cluster_nodate,
	title = {Multi-cluster use cases {\textbar} {Fleet} management},
	url = {https://cloud.google.com/anthos/fleet-management/docs/multi-cluster-use-cases},
	language = {en},
	urldate = {2024-01-02},
	journal = {Google Cloud},
	file = {Snapshot:/Users/hamza/Zotero/storage/MXALW9YR/multi-cluster-use-cases.html:text/html},
}

@article{feng_scaling_nodate,
	title = {Scaling {Large} {Production} {Clusters} with {Partitioned} {Synchronization}},
	abstract = {The scale of computer clusters has grown signiﬁcantly in recent years. Today, a cluster may have 100 thousand machines and execute billions of tasks, especially short tasks, each day. As a result, the scheduler, which manages resource utilization in a cluster, also needs to be upgraded to work at a much larger scale. However, upgrading the scheduler — a central system component — in a large production cluster is a daunting task as we need to ensure the cluster’s stability and robustness, e.g., user transparency should be guaranteed, and other cluster components and the existing scheduling policies need to remain unchanged. We investigated existing scheduler designs and found that most cannot handle the scale of our production clusters or may endanger their robustness. We analyzed one most suitable design that follows a sharedstate architecture, and its limitations led us to a ﬁne-grained staleness-aware state sharing design, called partitioned synchronization (ParSync). ParSync features the simplicity required for maintaining the robustness of a production cluster, while achieving high scheduling efﬁciency and quality in scaling. ParSync has been deployed and is running stably in our production clusters.},
	language = {en},
	author = {Feng, Yihui and Liu, Zhi and Zhao, Yunjian and Jin, Tatiana and Wu, Yidi and Zhang, Yang and Cheng, James and Li, Chao and Guan, Tao},
	file = {Feng et al. - Scaling Large Production Clusters with Partitioned.pdf:/Users/hamza/Zotero/storage/C23RPDG2/Feng et al. - Scaling Large Production Clusters with Partitioned.pdf:application/pdf},
}

@misc{noauthor_kubernetes_nodate,
	title = {Kubernetes {Multi}-{Cluster}: {Why} and {When} {To} {Use} {Them}},
	shorttitle = {Kubernetes {Multi}-{Cluster}},
	url = {https://www.qovery.com/blog/kubernetes-multi-cluster-why-and-when-to-use-them/},
	abstract = {Over the years, Kubernetes has stood out as one of the best platforms for container orchestration. However, managing Kubernetes is still complex. The first question which comes to mind is whether we should use a single cluster or a multi-cluster for Kubernetes. In many cases, a single cluster is not enough to manage the load efficiently across all components. As a result, we need more than one cluster for a better division of workload and resources, hence the need for a multi-cluster solution. In this article, we will discuss in detail multi-cluster Kubernetes, why it is used, and when we should prefer it over single-cluster.},
	language = {en},
	urldate = {2024-01-02},
	file = {Snapshot:/Users/hamza/Zotero/storage/M7YSLLAX/kubernetes-multi-cluster-why-and-when-to-use-them.html:text/html},
}

@misc{noauthor_opentelemetry_nodate,
	title = {{OpenTelemetry}},
	url = {https://opentelemetry.io/},
	abstract = {High-quality, ubiquitous, and portable telemetry to enable effective observability},
	language = {en},
	urldate = {2024-01-02},
	journal = {OpenTelemetry},
	file = {Snapshot:/Users/hamza/Zotero/storage/HSEKPIZZ/opentelemetry.io.html:text/html},
}

@misc{noauthor_jaeger_nodate,
	title = {Jaeger: open source, distributed tracing platform},
	shorttitle = {Jaeger},
	url = {https://www.jaegertracing.io/},
	abstract = {Monitor and troubleshoot workflows in complex distributed systems},
	language = {en},
	urldate = {2024-01-02},
	file = {Snapshot:/Users/hamza/Zotero/storage/9BB93L5B/www.jaegertracing.io.html:text/html},
}

@misc{noauthor_cluster-dataclusterdata2019md_nodate,
	title = {cluster-data/{ClusterData2019}.md at master · google/cluster-data},
	url = {https://github.com/google/cluster-data/blob/master/ClusterData2019.md},
	language = {en},
	urldate = {2024-01-02},
	file = {Snapshot:/Users/hamza/Zotero/storage/7U7NWV8L/ClusterData2019.html:text/html},
}

@misc{noauthor_alibabaclusterdata_2023,
	title = {alibaba/clusterdata},
	url = {https://github.com/alibaba/clusterdata},
	abstract = {cluster data collected from production clusters in Alibaba for cluster management research},
	urldate = {2024-01-02},
	publisher = {Alibaba},
	month = dec,
	year = {2023},
	note = {original-date: 2017-09-05T03:16:34Z},
	keywords = {dataset},
}

@article{qiao_pollux_nodate,
	title = {Pollux: {Co}-adaptive {Cluster} {Scheduling} for {Goodput}-{Optimized} {Deep} {Learning}},
	abstract = {Pollux improves scheduling performance in deep learning (DL) clusters by adaptively co-optimizing inter-dependent factors both at the per-job level and at the cluster-wide level. Most existing schedulers expect users to specify the number of resources for each job, often leading to inefﬁcient resource use. Some recent schedulers choose job resources for users, but do so without awareness of how DL training can be re-optimized to better utilize the provided resources.},
	language = {en},
	author = {Qiao, Aurick and Choe, Sang Keun and Subramanya, Suhas Jayaram and Neiswanger, Willie and Ho, Qirong and Zhang, Hao and Ganger, Gregory R and Xing, Eric P},
	file = {Qiao et al. - Pollux Co-adaptive Cluster Scheduling for Goodput.pdf:/Users/hamza/Zotero/storage/42SIW9A6/Qiao et al. - Pollux Co-adaptive Cluster Scheduling for Goodput.pdf:application/pdf},
}

@article{weng_beware_nodate,
	title = {Beware of {Fragmentation}: {Scheduling} {GPU}-{Sharing} {Workloads} with {Fragmentation} {Gradient} {Descent}},
	abstract = {Large tech companies are piling up a massive number of GPUs in their server fleets to run diverse machine learning (ML) workloads. However, these expensive devices often suffer from significant underutilization. To tackle this issue, GPU sharing techniques have been developed to enable multiple ML tasks to run on a single GPU. Nevertheless, our analysis of Alibaba production traces reveals that allocating partial GPUs can result in severe GPU fragmentation in large clusters, leaving hundreds of GPUs unable to be allocated. Existing resource packing algorithms fall short in addressing this problem, as GPU sharing mandates a new scheduling formulation beyond the classic bin packing.},
	language = {en},
	author = {Weng, Qizhen and Yang, Lingyun and Yu, Yinghao and Wang, Wei and Tang, Xiaochuan and Yang, Guodong and Zhang, Liping},
	file = {Weng et al. - Beware of Fragmentation Scheduling GPU-Sharing Wo.pdf:/Users/hamza/Zotero/storage/M85ZZUNI/Weng et al. - Beware of Fragmentation Scheduling GPU-Sharing Wo.pdf:application/pdf},
}

@inproceedings{delgado_job-aware_2016,
	address = {Santa Clara CA USA},
	title = {Job-aware {Scheduling} in {Eagle}: {Divide} and {Stick} to {Your} {Probes}},
	isbn = {978-1-4503-4525-5},
	shorttitle = {Job-aware {Scheduling} in {Eagle}},
	url = {https://dl.acm.org/doi/10.1145/2987550.2987563},
	doi = {10.1145/2987550.2987563},
	language = {en},
	urldate = {2024-01-11},
	booktitle = {Proceedings of the {Seventh} {ACM} {Symposium} on {Cloud} {Computing}},
	publisher = {ACM},
	author = {Delgado, Pamela and Didona, Diego and Dinu, Florin and Zwaenepoel, Willy},
	month = oct,
	year = {2016},
	pages = {497--509},
	file = {Full Text PDF:/Users/hamza/Zotero/storage/CYZ8H33Q/Delgado et al. - 2016 - Job-aware Scheduling in Eagle Divide and Stick to.pdf:application/pdf},
}

@article{feng_scaling_nodate-1,
	title = {Scaling {Large} {Production} {Clusters} with {Partitioned} {Synchronization}},
	abstract = {The scale of computer clusters has grown signiﬁcantly in recent years. Today, a cluster may have 100 thousand machines and execute billions of tasks, especially short tasks, each day. As a result, the scheduler, which manages resource utilization in a cluster, also needs to be upgraded to work at a much larger scale. However, upgrading the scheduler — a central system component — in a large production cluster is a daunting task as we need to ensure the cluster’s stability and robustness, e.g., user transparency should be guaranteed, and other cluster components and the existing scheduling policies need to remain unchanged. We investigated existing scheduler designs and found that most cannot handle the scale of our production clusters or may endanger their robustness. We analyzed one most suitable design that follows a sharedstate architecture, and its limitations led us to a ﬁne-grained staleness-aware state sharing design, called partitioned synchronization (ParSync). ParSync features the simplicity required for maintaining the robustness of a production cluster, while achieving high scheduling efﬁciency and quality in scaling. ParSync has been deployed and is running stably in our production clusters.},
	language = {en},
	author = {Feng, Yihui and Liu, Zhi and Zhao, Yunjian and Jin, Tatiana and Wu, Yidi and Zhang, Yang and Cheng, James and Li, Chao and Guan, Tao},
	file = {Feng et al. - Scaling Large Production Clusters with Partitioned.pdf:/Users/hamza/Zotero/storage/6D4572YC/Feng et al. - Scaling Large Production Clusters with Partitioned.pdf:application/pdf},
}

@inproceedings{lu_understanding_2023,
	address = {Rome Italy},
	title = {Understanding and {Optimizing} {Workloads} for {Unified} {Resource} {Management} in {Large} {Cloud} {Platforms}},
	isbn = {978-1-4503-9487-1},
	url = {https://dl.acm.org/doi/10.1145/3552326.3587437},
	doi = {10.1145/3552326.3587437},
	language = {en},
	urldate = {2024-02-21},
	booktitle = {Proceedings of the {Eighteenth} {European} {Conference} on {Computer} {Systems}},
	publisher = {ACM},
	author = {Lu, Chengzhi and Xu, Huanle and Ye, Kejiang and Xu, Guoyao and Zhang, Liping and Yang, Guodong and Xu, Chengzhong},
	month = may,
	year = {2023},
	pages = {416--432},
	file = {Full Text PDF:/Users/hamza/Zotero/storage/X623TC8F/Lu et al. - 2023 - Understanding and Optimizing Workloads for Unified.pdf:application/pdf},
}

@misc{noauthor_swarm_0100,
	title = {Swarm mode overview},
	url = {https://docs.docker.com/engine/swarm/},
	abstract = {Docker Engine Swarm mode overview},
	language = {en},
	urldate = {2024-07-28},
	journal = {Docker Documentation},
	year = {0100},
	file = {Snapshot:/Users/hamza/Zotero/storage/TKEBQ7YB/swarm.html:text/html},
}

@misc{noauthor_fleet_nodate,
	title = {Fleet management  {\textbar}  {Google} {Kubernetes} {Engine} ({GKE})  {\textbar}  {Google} {Cloud}},
	url = {https://cloud.google.com/kubernetes-engine/docs/fleets-overview},
	urldate = {2024-07-29},
	file = {Fleet management  |  Google Kubernetes Engine (GKE)  |  Google Cloud:/Users/hamza/Zotero/storage/2P9L3CWT/fleets-overview.html:text/html},
}

@misc{noauthor_multi-cluster_nodate-1,
	title = {Multi-cluster management for {Kubernetes} with {Cluster} {API} and {Argo} {CD} {\textbar} {Containers}},
	url = {https://aws.amazon.com/blogs/containers/multi-cluster-management-for-kubernetes-with-cluster-api-and-argo-cd/},
	urldate = {2024-07-29},
	file = {Multi-cluster management for Kubernetes with Cluster API and Argo CD | Containers:/Users/hamza/Zotero/storage/4EGBFB4M/multi-cluster-management-for-kubernetes-with-cluster-api-and-argo-cd.html:text/html},
}

@misc{noauthor_azure_nodate,
	title = {Azure {Kubernetes} {Fleet} {Manager} – {Cluster} {Management} {\textbar} {Microsoft} {Azure}},
	url = {https://azure.microsoft.com/en-us/products/kubernetes-fleet-manager},
	abstract = {Azure Kubernetes Fleet Manager simplifies multicluster management for Azure Kubernetes Service and Azure Arc-enabled Kubernetes clusters.},
	language = {en-US},
	urldate = {2024-07-29},
	file = {Snapshot:/Users/hamza/Zotero/storage/W6G5DFFA/kubernetes-fleet-manager.html:text/html},
}

@misc{noauthor_multi-region_nodate,
	title = {Multi-region federation {\textbar} {Nomad} {\textbar} {HashiCorp} {Developer}},
	url = {https://developer.hashicorp.com/nomad/tutorials/manage-clusters/federation},
	urldate = {2024-07-29},
	file = {Multi-region federation | Nomad | HashiCorp Developer:/Users/hamza/Zotero/storage/IVXELRV2/federation.html:text/html},
}

@misc{noauthor_cluster_nodate,
	title = {Cluster {Mesh}},
	url = {https://cilium.io/use-cases/cluster-mesh/},
	urldate = {2024-07-29},
	file = {Cluster Mesh:/Users/hamza/Zotero/storage/5HZJ7PM8/cluster-mesh.html:text/html},
}

@misc{noauthor_install_nodate,
	title = {Install {Multicluster}},
	url = {https://istio.io/latest/docs/setup/install/multicluster/},
	abstract = {Install an Istio mesh across multiple Kubernetes clusters.},
	language = {en},
	urldate = {2024-07-29},
	journal = {Istio},
	file = {Snapshot:/Users/hamza/Zotero/storage/HCMXJQ4P/multicluster.html:text/html},
}

@misc{noauthor__nodate,
	title = {:: {Submariner} k8s project documentation website},
	url = {https://submariner.io/},
	urldate = {2024-07-29},
	file = {\:\: Submariner k8s project documentation website:/Users/hamza/Zotero/storage/CPE47ZMG/submariner.io.html:text/html},
}

@misc{noauthor_skupper_nodate,
	title = {Skupper - {Multicloud} communication},
	url = {https://skupper.io/},
	urldate = {2024-07-29},
}

@article{yang_skypilot_nodate,
	title = {{SkyPilot}: {An} {Intercloud} {Broker} for {Sky} {Computing}},
	abstract = {To comply with the increasing number of government regulations about data placement and processing, and to protect themselves against major cloud outages, many users want the ability to easily migrate their workloads between clouds. In this paper we propose doing so not by imposing uniform and comprehensive standards, but by creating a ﬁne-grained two-sided market via an intercloud broker. These brokers will allow users to view the cloud ecosystem not just as a collection of individual and largely incompatible clouds but as a more integrated Sky of Computing. We describe the design and implementation of an intercloud broker, named SkyPilot, evaluate its beneﬁts, and report on its real-world usage.},
	language = {en},
	author = {Yang, Zongheng and Wu, Zhanghao and Luo, Michael and Chiang, Wei-Lin and Bhardwaj, Romil and Kwon, Woosuk and Zhuang, Siyuan and Luan, Frank Sifei and Mittal, Gautam and Shenker, Scott and Stoica, Ion},
	file = {Yang et al. - SkyPilot An Intercloud Broker for Sky Computing.pdf:/Users/hamza/Zotero/storage/TC6AK4T5/Yang et al. - SkyPilot An Intercloud Broker for Sky Computing.pdf:application/pdf},
}

@article{iorio_computing_2023,
	title = {Computing {Without} {Borders}: {The} {Way} {Towards} {Liquid} {Computing}},
	volume = {11},
	issn = {2168-7161, 2372-0018},
	shorttitle = {Computing {Without} {Borders}},
	url = {http://arxiv.org/abs/2204.05710},
	doi = {10.1109/TCC.2022.3229163},
	abstract = {Despite the de-facto technological uniformity fostered by the cloud and edge computing paradigms, resource fragmentation across isolated clusters hinders the dynamism in application placement, leading to suboptimal performance and operational complexity. Building upon and extending these paradigms, we propose a novel approach envisioning a transparent continuum of resources and services on top of the underlying fragmented infrastructure, called liquid computing. Fully decentralized, multi-ownership-oriented and intent-driven, it enables an overarching abstraction for improved applications execution, while at the same time opening up for new scenarios, including resource sharing and brokering. Following the above vision, we present liqo, an open-source project that materializes this approach through the creation of dynamic and seamless Kubernetes multi-cluster topologies. Extensive experimental evaluations have shown its effectiveness in different contexts, both in terms of Kubernetes overhead and compared to other opensource alternatives.},
	language = {en},
	number = {3},
	urldate = {2024-07-29},
	journal = {IEEE Transactions on Cloud Computing},
	author = {Iorio, Marco and Risso, Fulvio and Palesandro, Alex and Camiciotti, Leonardo and Manzalini, Antonio},
	month = jul,
	year = {2023},
	note = {arXiv:2204.05710 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {2820--2838},
	annote = {Comment: 18 pages, 12 figures},
	file = {Iorio et al. - 2023 - Computing Without Borders The Way Towards Liquid .pdf:/Users/hamza/Zotero/storage/CWAXKRED/Iorio et al. - 2023 - Computing Without Borders The Way Towards Liquid .pdf:application/pdf},
}

@misc{noauthor_liqo_nodate,
	title = {Liqo},
	url = {https://liqo.io/},
	urldate = {2024-07-29},
	file = {Liqo:/Users/hamza/Zotero/storage/3NNXVWUY/liqo.io.html:text/html},
}

@inproceedings{boutin_apollo_2014,
	address = {Broomfield, CO},
	title = {Apollo: {Scalable} and {Coordinated} {Scheduling} for {Cloud}-{Scale} {Computing}},
	isbn = {978-1-931971-16-4},
	url = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/boutin},
	booktitle = {11th {USENIX} {Symposium} on {Operating} {Systems} {Design} and {Implementation} ({OSDI} 14)},
	publisher = {USENIX Association},
	author = {Boutin, Eric and Ekanayake, Jaliya and Lin, Wei and Shi, Bing and Zhou, Jingren and Qian, Zhengping and Wu, Ming and Zhou, Lidong},
	month = oct,
	year = {2014},
	pages = {285--300},
}

@inproceedings{schwarzkopf_omega_2013,
	address = {Prague, Czech Republic},
	title = {Omega: flexible, scalable schedulers for large compute clusters},
	url = {http://eurosys2013.tudos.org/wp-content/uploads/2013/paper/Schwarzkopf.pdf},
	booktitle = {{SIGOPS} {European} {Conference} on {Computer} {Systems} ({EuroSys})},
	author = {Schwarzkopf, Malte and Konwinski, Andy and Abd-El-Malek, Michael and Wilkes, John},
	year = {2013},
	pages = {351--364},
}

@inproceedings{garg_workload_2017,
	title = {Workload performance and interference on containers},
	doi = {10.1109/UIC-ATC.2017.8397647},
	booktitle = {2017 {IEEE} {SmartWorld}, {Ubiquitous} {Intelligence} \& {Computing}, {Advanced} \& {Trusted} {Computed}, {Scalable} {Computing} \& {Communications}, {Cloud} \& {Big} {Data} {Computing}, {Internet} of {People} and {Smart} {City} {Innovation} ({SmartWorld}/{SCALCOM}/{UIC}/{ATC}/{CBDCom}/{IOP}/{SCI})},
	author = {Garg, Surya Kant and Lakshmi, J.},
	year = {2017},
	keywords = {Resource management, Servers, Benchmark testing, Containers, Interference, Kernel, Linux},
	pages = {1--6},
}

@inproceedings{karanasos_mercury_2015,
	address = {Santa Clara, CA},
	title = {Mercury: {Hybrid} {Centralized} and {Distributed} {Scheduling} in {Large} {Shared} {Clusters}},
	isbn = {978-1-931971-22-5},
	url = {https://www.usenix.org/conference/atc15/technical-session/presentation/karanasos},
	booktitle = {2015 {USENIX} {Annual} {Technical} {Conference} ({USENIX} {ATC} 15)},
	publisher = {USENIX Association},
	author = {Karanasos, Konstantinos and Rao, Sriram and Curino, Carlo and Douglas, Chris and Chaliparambil, Kishore and Fumarola, Giovanni Matteo and Heddaya, Solom and Ramakrishnan, Raghu and Sakalanaga, Sarvesh},
	month = jul,
	year = {2015},
	pages = {485--497},
}

@inproceedings{delgado_hawk_2015,
	address = {Santa Clara, CA},
	title = {Hawk: {Hybrid} {Datacenter} {Scheduling}},
	isbn = {978-1-931971-22-5},
	url = {https://www.usenix.org/conference/atc15/technical-session/presentation/delgado},
	booktitle = {2015 {USENIX} {Annual} {Technical} {Conference} ({USENIX} {ATC} 15)},
	publisher = {USENIX Association},
	author = {Delgado, Pamela and Dinu, Florin and Kermarrec, Anne-Marie and Zwaenepoel, Willy},
	month = jul,
	year = {2015},
	pages = {499--510},
}

@misc{noauthor_dynamic_nodate-1,
	title = {Dynamic {Kubernetes} {Cluster} {Scaling} at {Airbnb} {\textbar} by {David} {Morrison} {\textbar} {The} {Airbnb} {Tech} {Blog} {\textbar} {Medium}},
	url = {https://medium.com/airbnb-engineering/dynamic-kubernetes-cluster-scaling-at-airbnb-d79ae3afa132},
	urldate = {2024-07-29},
	file = {Dynamic Kubernetes Cluster Scaling at Airbnb | by David Morrison | The Airbnb Tech Blog | Medium:/Users/hamza/Zotero/storage/RWLF6HMD/dynamic-kubernetes-cluster-scaling-at-airbnb-d79ae3afa132.html:text/html},
}

@misc{noauthor_welcome_nodate,
	title = {Welcome to the next tech revolution: {Liquid} computing},
	shorttitle = {Welcome to the next tech revolution},
	url = {https://www.infoworld.com/article/2180167/welcome-to-the-next-tech-revolution-liquid-computing.html},
	abstract = {The Handoff feature in iOS 8 and OS X Yosemite is the first taste of truly contextual computing},
	language = {en-US},
	urldate = {2024-07-29},
	journal = {InfoWorld},
	file = {Snapshot:/Users/hamza/Zotero/storage/IT5BYPIP/welcome-to-the-next-tech-revolution-liquid-computing.html:text/html},
}

@misc{noauthor_nomad_nodate,
	title = {Nomad by {HashiCorp}},
	url = {https://www.nomadproject.io/},
	abstract = {Nomad is a simple and flexible scheduler and orchestrator for managing containers and non-containerized applications across on-prem and clouds at scale.},
	language = {en},
	urldate = {2024-07-29},
	journal = {Nomad by HashiCorp},
	file = {Snapshot:/Users/hamza/Zotero/storage/MFV8Y5FU/www.nomadproject.io.html:text/html},
}

@misc{noauthor_considerations_nodate,
	title = {Considerations for large clusters},
	url = {https://kubernetes.io/docs/setup/best-practices/cluster-large/},
	abstract = {A cluster is a set of nodes (physical or virtual machines) running Kubernetes agents, managed by the control plane. Kubernetes v1.30 supports clusters with up to 5,000 nodes. More specifically, Kubernetes is designed to accommodate configurations that meet all of the following criteria:
No more than 110 pods per node No more than 5,000 nodes No more than 150,000 total pods No more than 300,000 total containers You can scale your cluster by adding or removing nodes.},
	language = {en},
	urldate = {2024-07-29},
	note = {Section: docs},
	file = {Snapshot:/Users/hamza/Zotero/storage/37PZMB9X/cluster-large.html:text/html},
}

@misc{noauthor_multi-cluster_nodate-2,
	title = {Multi-cluster communication {\textbar} {Linkerd}},
	url = {https://linkerd.io/2.15/features/multicluster/},
	urldate = {2024-07-29},
	file = {Multi-cluster communication | Linkerd:/Users/hamza/Zotero/storage/SXTMJBS9/multicluster.html:text/html},
}

@inproceedings{duplyakin_design_2019,
	address = {Renton, WA},
	title = {The {Design} and {Operation} of {CloudLab}},
	isbn = {978-1-939133-03-8},
	url = {https://www.usenix.org/conference/atc19/presentation/duplyakin},
	booktitle = {2019 {USENIX} {Annual} {Technical} {Conference} ({USENIX} {ATC} 19)},
	publisher = {USENIX Association},
	author = {Duplyakin, Dmitry and Ricci, Robert and Maricq, Aleksander and Wong, Gary and Duerig, Jonathon and Eide, Eric and Stoller, Leigh and Hibler, Mike and Johnson, David and Webb, Kirk and Akella, Aditya and Wang, Kuangching and Ricart, Glenn and Landweber, Larry and Elliott, Chip and Zink, Michael and Cecchet, Emmanuel and Kar, Snigdhaswin and Mishra, Prabodh},
	month = jul,
	year = {2019},
	pages = {1--14},
}


@webpage{azure_pricing,
  abstract = {Get pricing info for Azure Cloud Services for deploying apps and APIs. No upfront costs. Pay as you go. FREE trial.},
    date-modified = {2024-11-24 22:30:52 -0700},
      language = {en},
        title = {Pricing - {Cloud} {Services} {\textbar} {Microsoft} {Azure}},
          url = {https://azure.microsoft.com/en-us/pricing/details/cloud-services/},
            urldate = {2024-07-25},
              bdsk-url-1 = {https://azure.microsoft.com/en-us/pricing/details/cloud-services/}}

